{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "two4two_leNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRT0k+g0o7dY6qZYmbcHSk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mschuessler/two4two/blob/trainKerasExample/examples/two4two_leNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdgP1RGA4hzg"
      },
      "source": [
        "import pathlib\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WVdllt_HB7V"
      },
      "source": [
        "# Two4two data training\n",
        "The notebook demonstrates how to train a modern LeNet CNN on a Dataset pregenerated with the [two4two Module](https://github.com/mschuessler/two4two).\n",
        "\n",
        "If you open this notebook in Colab please make sure to request a GPU Instance. Training times will be excessively slow otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPVYMHZkJuj8"
      },
      "source": [
        "# Mounting Google drive to save trained model later\n",
        "Since colab is a free resource it runtimes are limited. Hence we want to save our model to reuse it late when our reserved instance terminates. To do this we mount a google drive. If you don't want to use Google drive or if you run this notebook on your own juypter notebook server please skip the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9FWWAq3J19A",
        "outputId": "490a805a-76d9-4fd0-8e7b-30babdfd2b95"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giYoHx93manY"
      },
      "source": [
        "We will now define the path to which or trained model will be saved. You may alter the name of the dictionary according to your preference. If you run this notebook outside of Google Colab just change this path to a local path of your notebook server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_h3e1rxmHuk"
      },
      "source": [
        "model_filepath = \"/content/gdrive/My Drive/two4two_example_model\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlYVeSFmm3xK"
      },
      "source": [
        "We will use the the callback functionality of keras to save our model whenever we achieved a new highest validation accuracy when training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4_PRQ0sKrY0"
      },
      "source": [
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=model_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk7DT8FzJcrA"
      },
      "source": [
        "# Defining model architecture / loading previously trained model\n",
        "The follwoing cell test wheter the dictionary where the model should be saved exist, if so we try to load it.\n",
        "\n",
        "If the directory does not exist we define a new model according to a modern LeNet architecture and complie it using the ADAM optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfhE_gqFkawN"
      },
      "source": [
        "trained_model_exists = os.path.exists(model_filepath)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxWnqQLbFarq"
      },
      "source": [
        "if trained_model_exists:\n",
        "  modernLenetModel = keras.models.load_model(model_filepath)\n",
        "else:\n",
        "  modernLenetModel = keras.models.Sequential([\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(2, activation=\"softmax\"),\n",
        "    ])\n",
        "  modernLenetModel.compile(loss=\"categorical_crossentropy\",\n",
        "                             optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XkRyErrlmTG"
      },
      "source": [
        "# Download example dataset\n",
        "In this example we will be using a pregenerated dataset package. It contains a larger dataset called \"medVarSpherObjColorBias\". This dataset contains two biases. The sphercitiy of the blocks as well as their color are somewhat predictive of the label peaky or stretchy. The package contains 40.000 images for training, 10.000 for validation and 1.000 for testing.\n",
        "\n",
        "It also contains the testing data of 1000 images each for three other datasets that contain only one or none of the two biases.\n",
        "\n",
        "Hence, we would expect that if we train a model on the dataset with two biases it should before worse on the other test sets, if the biases are used by the model for its predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgTNO0q6JaAa"
      },
      "source": [
        "datasets = [\"medVarSpherObjColorBias\", \"medVar\",\t\"medVarObjColorBias\", \"medVarSpherBias\"]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAN-gfkP4jnx",
        "outputId": "9602ce38-fc66-4e42-8c50-d6dc12678cba"
      },
      "source": [
        "dataset_url = data_dir = keras.utils.get_file(\n",
        "    origin=\"https://f001.backblazeb2.com/file/two4two/datasets_models/example_dataset.tar.gz\",\n",
        "    fname=\"example_dataset\",\n",
        "    untar=True\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://f001.backblazeb2.com/file/two4two/datasets_models/example_dataset.tar.gz\n",
            "1214414848/1214411699 [==============================] - 45s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-f2GEpIJO0V"
      },
      "source": [
        "# Reading dataframe from jsonl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Qi_MIL4r0p"
      },
      "source": [
        "train_dir = os.path.join(data_dir, datasets[0], \"train\")\n",
        "train_df = pd.read_json(os.path.join(train_dir, \"parameters.jsonl\"), lines=True)\n",
        "train_df[\"filename\"] = train_df[\"id\"] + \".png\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmx53eyxFPlj"
      },
      "source": [
        "valid_dir = os.path.join(data_dir, datasets[0], \"validation\")\n",
        "valid_df = pd.read_json(os.path.join(valid_dir, \"parameters.jsonl\"), lines=True)\n",
        "valid_df[\"filename\"] = valid_df[\"id\"] + \".png\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jgyTApTJUY2"
      },
      "source": [
        "# Creating Datagenerator from dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNaYUXbBFXJG",
        "outputId": "d5db8aa0-0e22-4b08-b536-74809898b0c9"
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "train_generator = datagen.flow_from_dataframe(dataframe=train_df, directory=train_dir,\n",
        "                                              x_col=\"filename\", y_col=\"obj_name\", batch_size=64)\n",
        "valid_generator = datagen.flow_from_dataframe(dataframe=valid_df, directory=valid_dir,\n",
        "                                              x_col=\"filename\", y_col=\"obj_name\", batch_size=64)\n",
        "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
        "STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 40000 validated image filenames belonging to 2 classes.\n",
            "Found 10000 validated image filenames belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkR0thUgJqNB"
      },
      "source": [
        "# Train Model\n",
        "We highly recommend to train the model at least for 30 or even better 45 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCeQePhAFvRy",
        "outputId": "8630c151-436d-4cf4-814f-a31e100b6957"
      },
      "source": [
        "modernLenetModel.fit(train_generator,\n",
        "                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                     validation_data=valid_generator,\n",
        "                     validation_steps=STEP_SIZE_VALID,\n",
        "                     epochs=45,\n",
        "                     callbacks = [model_checkpoint_callback]\n",
        "                     )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "625/625 [==============================] - 152s 189ms/step - loss: 0.4403 - accuracy: 0.7981 - val_loss: 0.4407 - val_accuracy: 0.7989\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 2/45\n",
            "625/625 [==============================] - 115s 183ms/step - loss: 0.3900 - accuracy: 0.8226 - val_loss: 0.3754 - val_accuracy: 0.8212\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 3/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.3557 - accuracy: 0.8403 - val_loss: 0.3204 - val_accuracy: 0.8587\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 4/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.3362 - accuracy: 0.8512 - val_loss: 0.2994 - val_accuracy: 0.8698\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 5/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.3224 - accuracy: 0.8564 - val_loss: 0.3076 - val_accuracy: 0.8679\n",
            "Epoch 6/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.3115 - accuracy: 0.8608 - val_loss: 0.2882 - val_accuracy: 0.8730\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 7/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.3057 - accuracy: 0.8648 - val_loss: 0.2813 - val_accuracy: 0.8757\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 8/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2966 - accuracy: 0.8701 - val_loss: 0.2872 - val_accuracy: 0.8763\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 9/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2893 - accuracy: 0.8726 - val_loss: 0.2744 - val_accuracy: 0.8801\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 10/45\n",
            "625/625 [==============================] - 114s 183ms/step - loss: 0.2851 - accuracy: 0.8741 - val_loss: 0.2703 - val_accuracy: 0.8796\n",
            "Epoch 11/45\n",
            "625/625 [==============================] - 114s 183ms/step - loss: 0.2778 - accuracy: 0.8775 - val_loss: 0.2618 - val_accuracy: 0.8871\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 12/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.2732 - accuracy: 0.8777 - val_loss: 0.2767 - val_accuracy: 0.8806\n",
            "Epoch 13/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2707 - accuracy: 0.8806 - val_loss: 0.2614 - val_accuracy: 0.8834\n",
            "Epoch 14/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2655 - accuracy: 0.8844 - val_loss: 0.2656 - val_accuracy: 0.8819\n",
            "Epoch 15/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2602 - accuracy: 0.8853 - val_loss: 0.2539 - val_accuracy: 0.8885\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 16/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2588 - accuracy: 0.8870 - val_loss: 0.2553 - val_accuracy: 0.8881\n",
            "Epoch 17/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2542 - accuracy: 0.8872 - val_loss: 0.2544 - val_accuracy: 0.8870\n",
            "Epoch 18/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2508 - accuracy: 0.8903 - val_loss: 0.2572 - val_accuracy: 0.8849\n",
            "Epoch 19/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2469 - accuracy: 0.8928 - val_loss: 0.2562 - val_accuracy: 0.8852\n",
            "Epoch 20/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.2445 - accuracy: 0.8929 - val_loss: 0.2483 - val_accuracy: 0.8886\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 21/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.2441 - accuracy: 0.8941 - val_loss: 0.2516 - val_accuracy: 0.8927\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/two4two_example_model/assets\n",
            "Epoch 22/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2376 - accuracy: 0.8967 - val_loss: 0.2560 - val_accuracy: 0.8863\n",
            "Epoch 23/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2387 - accuracy: 0.8958 - val_loss: 0.2483 - val_accuracy: 0.8901\n",
            "Epoch 24/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2349 - accuracy: 0.8984 - val_loss: 0.2631 - val_accuracy: 0.8851\n",
            "Epoch 25/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2324 - accuracy: 0.8997 - val_loss: 0.2486 - val_accuracy: 0.8886\n",
            "Epoch 26/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2309 - accuracy: 0.9024 - val_loss: 0.2607 - val_accuracy: 0.8816\n",
            "Epoch 27/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2272 - accuracy: 0.9017 - val_loss: 0.2464 - val_accuracy: 0.8871\n",
            "Epoch 28/45\n",
            "625/625 [==============================] - 112s 180ms/step - loss: 0.2269 - accuracy: 0.9013 - val_loss: 0.2499 - val_accuracy: 0.8886\n",
            "Epoch 29/45\n",
            "625/625 [==============================] - 112s 180ms/step - loss: 0.2217 - accuracy: 0.9042 - val_loss: 0.2532 - val_accuracy: 0.8834\n",
            "Epoch 30/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2199 - accuracy: 0.9040 - val_loss: 0.2491 - val_accuracy: 0.8891\n",
            "Epoch 31/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2182 - accuracy: 0.9061 - val_loss: 0.2603 - val_accuracy: 0.8875\n",
            "Epoch 32/45\n",
            "625/625 [==============================] - 112s 179ms/step - loss: 0.2167 - accuracy: 0.9074 - val_loss: 0.2487 - val_accuracy: 0.8882\n",
            "Epoch 33/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2149 - accuracy: 0.9072 - val_loss: 0.2551 - val_accuracy: 0.8870\n",
            "Epoch 34/45\n",
            "625/625 [==============================] - 112s 180ms/step - loss: 0.2089 - accuracy: 0.9114 - val_loss: 0.2471 - val_accuracy: 0.8925\n",
            "Epoch 35/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2098 - accuracy: 0.9100 - val_loss: 0.2491 - val_accuracy: 0.8869\n",
            "Epoch 36/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2098 - accuracy: 0.9085 - val_loss: 0.2551 - val_accuracy: 0.8853\n",
            "Epoch 37/45\n",
            "625/625 [==============================] - 113s 180ms/step - loss: 0.2095 - accuracy: 0.9109 - val_loss: 0.2601 - val_accuracy: 0.8904\n",
            "Epoch 38/45\n",
            "625/625 [==============================] - 114s 183ms/step - loss: 0.2056 - accuracy: 0.9110 - val_loss: 0.2501 - val_accuracy: 0.8895\n",
            "Epoch 39/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2023 - accuracy: 0.9142 - val_loss: 0.2507 - val_accuracy: 0.8920\n",
            "Epoch 40/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2038 - accuracy: 0.9126 - val_loss: 0.2530 - val_accuracy: 0.8905\n",
            "Epoch 41/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.2014 - accuracy: 0.9154 - val_loss: 0.2534 - val_accuracy: 0.8886\n",
            "Epoch 42/45\n",
            "625/625 [==============================] - 113s 181ms/step - loss: 0.2019 - accuracy: 0.9134 - val_loss: 0.2531 - val_accuracy: 0.8900\n",
            "Epoch 43/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.1961 - accuracy: 0.9159 - val_loss: 0.2569 - val_accuracy: 0.8880\n",
            "Epoch 44/45\n",
            "625/625 [==============================] - 114s 182ms/step - loss: 0.1942 - accuracy: 0.9161 - val_loss: 0.2590 - val_accuracy: 0.8878\n",
            "Epoch 45/45\n",
            "625/625 [==============================] - 114s 183ms/step - loss: 0.1962 - accuracy: 0.9154 - val_loss: 0.2605 - val_accuracy: 0.8882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f8c1c4290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bstPOz7spG57"
      },
      "source": [
        "# Evaluating the model\n",
        "We now use our trained model on the test sets of all datasets. As expected the model perfroms worse on the models were not all biases are present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqkiCAWvXq_T",
        "outputId": "7b9b99f9-796b-46a1-c8f8-bcaa2eebf61f"
      },
      "source": [
        "for dataset_name in datasets:\n",
        "        test_dir = os.path.join(data_dir, dataset_name, \"test\")\n",
        "        test_df = pd.read_json(os.path.join(test_dir, \"parameters.jsonl\"), lines=True)\n",
        "        test_df[\"filename\"] = test_df[\"id\"] + \".png\"\n",
        "\n",
        "        datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "        test_generator = datagen.flow_from_dataframe(dataframe=test_df, directory=test_dir,\n",
        "                                                     x_col=\"filename\", y_col=\"obj_name\",\n",
        "                                                     batch_size=64)\n",
        "\n",
        "        print(\"Evaljateing on \" + dataset_name)\n",
        "        modernLenetModel.evaluate(test_generator)[1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 validated image filenames belonging to 2 classes.\n",
            "Evaljateing on medVarSpherObjColorBias\n",
            "16/16 [==============================] - 3s 197ms/step - loss: 0.2185 - accuracy: 0.9110\n",
            "Found 1000 validated image filenames belonging to 2 classes.\n",
            "Evaljateing on medVar\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 0.4988 - accuracy: 0.8100\n",
            "Found 1000 validated image filenames belonging to 2 classes.\n",
            "Evaljateing on medVarObjColorBias\n",
            "16/16 [==============================] - 3s 167ms/step - loss: 0.5059 - accuracy: 0.8040\n",
            "Found 1000 validated image filenames belonging to 2 classes.\n",
            "Evaljateing on medVarSpherBias\n",
            "16/16 [==============================] - 3s 167ms/step - loss: 0.2185 - accuracy: 0.9110\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}